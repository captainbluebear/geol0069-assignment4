{"cells":[{"cell_type":"markdown","metadata":{"id":"oDpD-emQs4wr"},"source":["# Unsupervised Learning with Altimetry Classification -- Assignment 4\n","\n","In this notebook are the following:\n","1. Classification of sea ice and leads using Gaussian Mixture Models (GMM) with altimetry data from Sentinel-3.\n","2. Graphs of the classified echoes to analyse them.\n","3. A confusion matrix comparing our GMM-classified results with the official ESA classification.\n","\n","Now, let's explore the application of these unsupervised methods to altimetry classification tasks, focusing specifically on distinguishing between sea ice and leads in Sentinel-3 altimetry dataset.\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"VV0BaidOpirG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! git add .\n","\n","! git commit -m \"all parts complete\"\n","\n","!git push -u origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mleNIt8fFYKF","executionInfo":{"status":"ok","timestamp":1708442337645,"user_tz":0,"elapsed":3503,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}},"outputId":"074dee4c-9797-4e5c-e800-6b863a1d6c39"},"execution_count":205,"outputs":[{"output_type":"stream","name":"stdout","text":["[main (root-commit) a701e78] first commit\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 Chapter1_Unsupervised_Learning_Methods_Fiona.ipynb\n","Enumerating objects: 3, done.\n","Counting objects: 100% (3/3), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (3/3), done.\n","Writing objects: 100% (3/3), 7.59 KiB | 647.00 KiB/s, done.\n","Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\n","To https://github.com/captainbluebear/geol0069-assignment4.git\n"," * [new branch]      main -> main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n"]}]},{"cell_type":"markdown","metadata":{"id":"5ifm3G8js4wx"},"source":["## Read in Functions Needed\n","Before delving into the modeling process, it's crucial to preprocess the data to ensure compatibility with our analytical models. This involves transforming the raw data into meaningful variables, such as peakniness and stack standard deviation (SSD), etc."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1708441949591,"user":{"displayName":"Fiona M","userId":"08618784324952634942"},"user_tz":0},"id":"OkVUCxu-vL_-"},"outputs":[],"source":["! pip install netCDF4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EpxMuwjms4wx","executionInfo":{"status":"aborted","timestamp":1708441949591,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"outputs":[],"source":["#\n","from netCDF4 import Dataset\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.interpolate import griddata\n","# from mpl_toolkits.basemap import Basemap\n","import numpy.ma as ma\n","import glob\n","from matplotlib.patches import Polygon\n","import scipy.spatial as spatial\n","from scipy.spatial import KDTree\n","\n","import pyproj\n","# import cartopy.crs as ccrs\n","from sklearn.cluster import KMeans, DBSCAN\n","from sklearn.preprocessing import StandardScaler,MinMaxScaler\n","from sklearn.mixture import GaussianMixture\n","from scipy.cluster.hierarchy import linkage, fcluster\n","\n","#=========================================================================================================\n","#===================================  SUBFUNCTIONS  ======================================================\n","#=========================================================================================================\n","#Credit: Isobel Lawrence\n","\n","#*args and **kwargs allow you to pass an unspecified number of arguments to a function,\n","#so when writing the function definition, you do not need to know how many arguments will be passed to your function\n","#**kwargs allows you to pass keyworded variable length of arguments to a function.\n","#You should use **kwargs if you want to handle named arguments in a function.\n","#double star allows us to pass through keyword arguments (and any number of them).\n","\n","# Feed it the echo, it calculates max/min and returns the value of the [false]? peakiness\n","def peakiness(waves, **kwargs):\n","\n","    \"finds peakiness of waveforms.\"\n","\n","    #print(\"Beginning peakiness\")\n","    # Kwargs are:\n","    #          wf_plots. specify a number n: wf_plots=n, to show the first n waveform plots. \\\n","\n","    import numpy as np\n","    import matplotlib\n","    import matplotlib.pyplot as plt\n","    import time\n","\n","    print(\"Running peakiness function...\")\n","\n","    size=np.shape(waves)[0] #.shape property is a tuple of length .ndim containing the length of each dimensions\n","                            #Tuple of array dimensions.\n","\n","    waves1=np.copy(waves)\n","\n","    if waves1.ndim == 1: #number of array dimensions\n","        print('only one waveform in file')\n","        waves2=waves1.reshape(1,np.size(waves1)) #numpy.reshape(a, newshape, order='C'), a=array to be reshaped\n","        waves1=waves2\n","\n","    # *args is used to send a non-keyworded variable length argument list to the function\n","    def by_row(waves, *args):\n","        \"calculate peakiness for each waveform\"\n","        maximum=np.nanmax(waves)\n","        if maximum > 0:\n","\n","            maximum_bin=np.where(waves==maximum)\n","            #print(maximum_bin)\n","            maximum_bin=maximum_bin[0][0]\n","            waves_128=waves[maximum_bin-50:maximum_bin+78]\n","\n","            waves=waves_128\n","\n","            noise_floor=np.nanmean(waves[10:20])\n","            where_above_nf=np.where(waves > noise_floor)\n","\n","            if np.shape(where_above_nf)[1] > 0:\n","                maximum=np.nanmax(waves[where_above_nf])\n","                total=np.sum(waves[where_above_nf])\n","                mean=np.nanmean(waves[where_above_nf])\n","                peaky=maximum/mean\n","\n","            else:\n","                peaky = np.nan\n","                maximum = np.nan\n","                total = np.nan\n","\n","        else:\n","            peaky = np.nan\n","            maximum = np.nan\n","            total = np.nan\n","\n","        if 'maxs' in args:\n","            return maximum\n","        if 'totals' in args:\n","            return total\n","        if 'peaky' in args:\n","            return peaky\n","\n","    peaky=np.apply_along_axis(by_row, 1, waves1, 'peaky') #numpy.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n","\n","    if 'wf_plots' in kwargs:\n","        maximums=np.apply_along_axis(by_row, 1, waves1, 'maxs')\n","        totals=np.apply_along_axis(by_row, 1, waves1, 'totals')\n","\n","        for i in range(0,kwargs['wf_plots']):\n","            if i == 0:\n","                print(\"Plotting first \"+str(kwargs['wf_plots'])+\" waveforms\")\n","\n","            plt.plot(waves1[i,:])#, a, col[i],label=label[i])\n","            plt.axhline(maximums[i], color='green')\n","            plt.axvline(10, color='r')\n","            plt.axvline(19, color='r')\n","            plt.xlabel('Bin (of 256)')\n","            plt.ylabel('Power')\n","            plt.text(5,maximums[i],\"maximum=\"+str(maximums[i]))\n","            plt.text(5,maximums[i]-2500,\"total=\"+str(totals[i]))\n","            plt.text(5,maximums[i]-5000,\"peakiness=\"+str(peaky[i]))\n","            plt.title('waveform '+str(i)+' of '+str(size)+'\\n. Noise floor average taken between red lines.')\n","            plt.show()\n","\n","\n","    return peaky\n","\n","#=========================================================================================================\n","#=========================================================================================================\n","#=========================================================================================================\n","\n","# This function gets out certain variables\n","def unpack_gpod(variable):\n","\n","    from scipy.interpolate import interp1d\n","\n","    time_1hz=SAR_data.variables['time_01'][:]\n","    time_20hz=SAR_data.variables['time_20_ku'][:]\n","    time_20hzC = SAR_data.variables['time_20_c'][:]\n","\n","    out=(SAR_data.variables[variable][:]).astype(float)  # convert from integer array to float.\n","\n","    #if ma.is_masked(dataset.variables[variable][:]) == True:\n","    #print(variable,'is masked. Removing mask and replacing masked values with nan')\n","    out=np.ma.filled(out, np.nan)\n","\n","    if len(out)==len(time_1hz):\n","\n","        print(variable,'is 1hz. Expanding to 20hz...')\n","        out = interp1d(time_1hz,out,fill_value=\"extrapolate\")(time_20hz)\n","\n","    if len(out)==len(time_20hzC):\n","        print(variable, 'is c band, expanding to 20hz ku band dimension')\n","        out = interp1d(time_20hzC,out,fill_value=\"extrapolate\")(time_20hz)\n","    return out\n","\n","\n","#=========================================================================================================\n","#=========================================================================================================\n","#=========================================================================================================\n","\n","# This function calculates stack-standard deviation\n","def calculate_SSD(RIP):\n","\n","    from scipy.optimize import curve_fit\n","    from scipy import asarray as ar,exp\n","    do_plot='Off'\n","\n","    def gaussian(x,a,x0,sigma):\n","            return a * np.exp(-(x - x0)**2 / (2 * sigma**2))\n","\n","    SSD=np.zeros(np.shape(RIP)[0])*np.nan\n","    x=np.arange(np.shape(RIP)[1])\n","\n","    for i in range(np.shape(RIP)[0]):\n","\n","        y=np.copy(RIP[i])\n","        y[(np.isnan(y)==True)]=0\n","\n","        if 'popt' in locals():\n","            del(popt,pcov)\n","\n","        SSD_calc=0.5*(np.sum(y**2)*np.sum(y**2)/np.sum(y**4))\n","        #print('SSD calculated from equation',SSD)\n","\n","        #n = len(x)\n","        mean_est = sum(x * y) / sum(y)\n","        sigma_est = np.sqrt(sum(y * (x - mean_est)**2) / sum(y))\n","        #print('est. mean',mean,'est. sigma',sigma_est)\n","\n","        try:\n","            popt,pcov = curve_fit(gaussian, x, y, p0=[max(y), mean_est, sigma_est],maxfev=10000)\n","        except RuntimeError as e:\n","            print(\"Gaussian SSD curve-fit error: \"+str(e))\n","            #plt.plot(y)\n","            #plt.show()\n","\n","        except TypeError as t:\n","            print(\"Gaussian SSD curve-fit error: \"+str(t))\n","\n","        if do_plot=='ON':\n","\n","            plt.plot(x,y)\n","            plt.plot(x,gaussian(x,*popt),'ro:',label='fit')\n","            plt.axvline(popt[1])\n","            plt.axvspan(popt[1]-popt[2], popt[1]+popt[2], alpha=0.15, color='Navy')\n","            plt.show()\n","\n","            print('popt',popt)\n","            print('curve fit SSD',popt[2])\n","\n","        if 'popt' in locals():\n","            SSD[i]=abs(popt[2])\n","\n","\n","    return SSD\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1708441949591,"user":{"displayName":"Fiona M","userId":"08618784324952634942"},"user_tz":0},"id":"T-t4GMyMs4wx"},"outputs":[],"source":["path = '/content/drive/MyDrive/UMD/Spring 24 UCL/GEOL0069/Week_4/' # You need to specify the path\n","\n","SAR_file='S3B_SR_2_LAN_SI_20190301T231304_20190301T233006_20230405T162425_1021_022_301______LN3_R_NT_005.SEN3' #This should be the ALTIMETRY SEN-3 data\n","print('overlapping SAR file is',SAR_file)\n","SAR_data=Dataset(path + SAR_file+'/enhanced_measurement.nc')\n","\n","# We are unpacking thresholds(?) provided by ESA. Classified as leads, ice, ocean, and [land & unclassified] (one group)\n","SAR_lat, SAR_lon, waves, sig_0, RIP, flag = unpack_gpod('lat_20_ku'), unpack_gpod('lon_20_ku'), unpack_gpod('waveform_20_ku'),unpack_gpod('sig0_water_20_ku'),unpack_gpod('rip_20_ku'),unpack_gpod('surf_type_class_20_ku') #unpack_gpod('Sigma0_20Hz')\n","SAR_index=np.arange(np.size(SAR_lat))\n","\n","find=np.where(SAR_lat >= -99999)#60\n","SAR_lat=SAR_lat[find]\n","SAR_lon=SAR_lon[find]\n","SAR_index=SAR_index[find]\n","waves=waves[find]\n","sig_0=sig_0[find]\n","RIP=RIP[find]\n","\n","PP=peakiness(waves)\n","SSD=calculate_SSD(RIP)\n","sig_0_np = np.array(sig_0)  # Replace [...] with your data\n","RIP_np = np.array(RIP)\n","PP_np = np.array(PP)\n","SSD_np = np.array(SSD)\n","\n","data = np.column_stack((sig_0_np,PP_np, SSD_np))\n","# Standardize the data\n","scaler = StandardScaler()\n","data_normalized = scaler.fit_transform(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1708441949591,"user":{"displayName":"Fiona M","userId":"08618784324952634942"},"user_tz":0},"id":"CCvllyvY091B"},"outputs":[],"source":["# We calculated for all 13,000 echos on this track.\n","# We have three indices, which are sigma-0, pulse-pickiness, and stack-standard deviation\n","data_normalized.shape"]},{"cell_type":"markdown","source":["There are some NaN values within the data. These values should be removed. In addition, let us only select bands containing flags for either sea ice or leads, as other flags are not useful."],"metadata":{"id":"4C7ctT5T43Hh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vb0vmy7Z86vj","executionInfo":{"status":"aborted","timestamp":1708441949591,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"outputs":[],"source":["# Makes it such that NaN values are ignored\n","flag_cleaned = flag[~np.isnan(data_normalized).any(axis=1)]\n","\n","nan_count = np.isnan(data_normalized).sum()\n","print(f\"Number of NaN values in the array: {nan_count}\")\n","data_cleaned = data_normalized[~np.isnan(data_normalized).any(axis=1)]\n","\n","# Only select bands that have a flag that is either sea ice or lead\n","waves_cleaned=waves[~np.isnan(data_normalized).any(axis=1)][(flag_cleaned==1)|(flag_cleaned==2)]"]},{"cell_type":"markdown","source":["## Examining ESA Data\n","\n","Taking a quick look at the data, we plot the normalized echoes divided by the sum of the values. The echoes are normalized by dividing each value by the sum of all values in the function. This ensures all echoes have a similar \"weight\" in the plot."],"metadata":{"id":"VIoiwMZW0JdR"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1708441949591,"user":{"displayName":"Fiona M","userId":"08618784324952634942"},"user_tz":0},"id":"lUWMIYnr88HL"},"outputs":[],"source":["# Plot of echoes where flag_cleaned == 1 (ice)\n","\n","functions_to_plot = waves[~np.isnan(data_normalized).any(axis=1)][(flag_cleaned==1)][::10] # Plot for every 10 of these echoes. We are plotting the \"1s\" aka ice\n","\n","threshold = 0.0006 # Threshold value for truncation (adjust as needed)\n","\n","# Plot each function\n","for i, function in enumerate(functions_to_plot):\n","    # Normalize function\n","    normalized_function = function/np.sum(function)\n","\n","    # Find the first index where function exceeds the threshold and truncate it\n","    truncation_index = np.where(normalized_function > threshold)[0][0]\n","    truncated_function = normalized_function[truncation_index:]\n","\n","    plt.plot(truncated_function)\n","\n","plt.xlabel('X-axis Label')\n","plt.ylabel('Y-axis Label')\n","plt.title('Plot of echoes where flag_cleaned == 1 (Ice)')\n","# plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"aborted","timestamp":1708441949591,"user":{"displayName":"Fiona M","userId":"08618784324952634942"},"user_tz":0},"id":"gEWyYTle9PJk"},"outputs":[],"source":["# Plot of echoes where flag_cleaned == 2 (leads)\n","\n","functions_to_plot = waves[~np.isnan(data_normalized).any(axis=1)][(flag_cleaned==2)][::10] #Here we are doing it for leads\n","\n","threshold = 0.0006 # Threshold value for truncation (adjust as needed)\n","\n","# Plot each function\n","for i, function in enumerate(functions_to_plot):\n","    # Normalize function\n","    normalized_function = function/np.sum(function)\n","\n","    # Find the first index where function exceeds the threshold and truncate it\n","    truncation_index = np.where(normalized_function > threshold)[0][0]\n","    truncated_function = normalized_function[truncation_index:]\n","\n","    plt.plot(truncated_function)\n","\n","plt.xlabel('X-axis Label')\n","plt.ylabel('Y-axis Label')\n","plt.title('Plot of echoes where flag_cleaned == 2 (Leads)')\n","# plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"atJponGTs4wy"},"source":["## Running Gaussian Mixture Model\n","\n","Now, let's proceed with running the GMM model as usual. We are trying to classify the echoes in leads and sea ice."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzVK6s2As4wy","executionInfo":{"status":"aborted","timestamp":1708441949592,"user_tz":0,"elapsed":7,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"outputs":[],"source":["# This is where we use the ML algorithm!\n","\n","gmm = GaussianMixture(n_components=2, random_state=0)\n","gmm.fit(data_cleaned[(flag_cleaned==1)|(flag_cleaned==2)])\n","clusters_gmm = gmm.predict(data_cleaned[(flag_cleaned==1)|(flag_cleaned==2)])"]},{"cell_type":"markdown","metadata":{"id":"iTGZN3DQs4wy"},"source":["We can inspect how many data points are there in each class of the clustering prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1708441949592,"user":{"displayName":"Fiona M","userId":"08618784324952634942"},"user_tz":0},"id":"UhZv0nqJs4wy"},"outputs":[],"source":["unique, counts = np.unique(clusters_gmm, return_counts=True)\n","class_counts = dict(zip(unique, counts))\n","\n","print(class_counts)"]},{"cell_type":"markdown","source":["## Examining GMM Data\n","\n","Now that the model has been run, let's take a look at the classification by using some graphs, starting with the predictions for ice."],"metadata":{"id":"Cs1kvlQA7EgS"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"aborted","timestamp":1708441949592,"user":{"displayName":"Fiona M","userId":"08618784324952634942"},"user_tz":0},"id":"ALNgTyU7ygpU"},"outputs":[],"source":["# Extract the functions where clusters_gmm is equal to 0 (ice) and truncate graph dynamically\n","\n","functions_to_plot = waves_cleaned[clusters_gmm == 0][::100]\n","\n","threshold = 0.0006 # Threshold value for truncation (adjust as needed)\n","\n","# Plot each function\n","for i, function in enumerate(functions_to_plot):\n","    # Normalize function\n","    normalized_function = function/np.sum(function)\n","\n","    # Find the first index where function exceeds the threshold and truncate it\n","    truncation_index = np.where(normalized_function > threshold)[0][0]\n","    truncated_function = normalized_function[truncation_index:]\n","\n","    plt.plot(truncated_function)\n","\n","plt.xlabel('X-axis Label')\n","plt.ylabel('Y-axis Label')\n","plt.title('Graph of functions where clusters_gmm == 0 (Ice)')\n","plt.show()"]},{"cell_type":"markdown","source":["Now we examine the graph for leads."],"metadata":{"id":"DczuU8hO8o2w"}},{"cell_type":"code","source":["# Extract the functions where clusters_gmm is equal to 1 (leads) and truncate graph dynamically\n","\n","functions_to_plot = waves_cleaned[clusters_gmm == 1][::100]\n","\n","threshold = 0.004 # Threshold value for truncation (adjust as needed)\n","\n","# Plot each function\n","for i, function in enumerate(functions_to_plot):\n","    # Normalize function\n","    normalized_function = function/np.sum(function)\n","\n","    # Find the first index where function exceeds the threshold and truncate it\n","    truncation_index = np.where(normalized_function > threshold)[0][0]\n","    truncated_function = normalized_function[truncation_index:]\n","\n","    plt.plot(truncated_function)\n","\n","plt.xlabel('X-axis Label')\n","plt.ylabel('Y-axis Label')\n","plt.title('Graph of functions where clusters_gmm == 1 (Leads)')\n","plt.show()"],"metadata":{"id":"LaTmJr2qTM6L","executionInfo":{"status":"aborted","timestamp":1708441949592,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Average Echo Shape and Standard Deviation\n","\n","In addition to these two graphs, let us produce an average echo shape as well as standard deviation for these two classes.\n"],"metadata":{"id":"9l7Rs4wm8sl5"}},{"cell_type":"code","source":["# Graph of average echo shape of ice and lead\n","\n","ice_avg = np.mean(waves_cleaned[clusters_gmm==0],axis=0)\n","lead_avg = np.mean(waves_cleaned[clusters_gmm==1],axis=0)\n","\n","avg_threshold = 10 # Modify as necessary. Determines graph cutoff at start\n","\n","# Find index at which to begin truncation\n","ice_truncation_index_avg = np.where(ice_avg > avg_threshold)[0][0]\n","lead_truncation_index_avg = np.where(lead_avg > avg_threshold)[0][0]\n","\n","# Perform truncation\n","truncated_ice_avg = ice_avg[ice_truncation_index_avg:]\n","truncated_lead_avg = lead_avg[lead_truncation_index_avg:]\n","\n","# Plot truncated averages\n","plt.plot(truncated_ice_avg, label='ice')\n","plt.plot(truncated_lead_avg, label='lead')\n","plt.title(\"Graph of Average Echo Shape for Lead and Ice\")\n","plt.legend()"],"metadata":{"id":"TuDYWhxKdx4x","executionInfo":{"status":"aborted","timestamp":1708441949592,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Graph of standard devation of echo for ice and lead\n","\n","ice_std = np.std(waves_cleaned[clusters_gmm==0],axis=0)\n","lead_std = np.std(waves_cleaned[clusters_gmm==1],axis=0)\n","\n","std_threshold = 10 # Modify as necessary. Determines graph cutoff at start\n","\n","# Find index at which to begin truncation\n","ice_truncation_index_std = np.where(ice_std > std_threshold)[0][0]\n","lead_truncation_index_std = np.where(lead_std > std_threshold)[0][0]\n","\n","# Perform truncation\n","truncated_ice_std = ice_std[ice_truncation_index_std:]\n","truncated_lead_std = lead_std[lead_truncation_index_std:]\n","\n","# Plot truncated averages\n","plt.plot(truncated_ice_std, label='ice')\n","plt.plot(truncated_lead_std, label='lead')\n","plt.title(\"Graph of Echo Standard Deviation for Lead and Ice\")\n","plt.legend()"],"metadata":{"id":"E2_wvWVypaSI","executionInfo":{"status":"aborted","timestamp":1708441949592,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confusion Matrix\n","\n","Now that we have our own GMM classification, it is good to perform a check of accuracy against the existing ESA classification. This can be done using a confusion matrix, similar to what was done in Week 1 of this course.\n","\n","The following code will quantify our echo classification against the ESA official classification using a confusion matrix. Some cleaning of the data must be done first, however, as the labels for GMM and ESA are assigned differently (ESA assigns 2 to leads and GMM assigns 1 to leads)."],"metadata":{"id":"HkQen7QK6SKE"}},{"cell_type":"code","source":["# Convert class_counts into a list containing 5013 0's and 1836 1's (0 = ice, 1 = lead)\n","gmm_labels = np.concatenate([[key] * value for key, value in class_counts.items()])"],"metadata":{"id":"FoEJC1wSyanx","executionInfo":{"status":"aborted","timestamp":1708441949592,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter out ice and lead flags (1 = ice and 2 = leads)\n","esa_labels = flag_cleaned[(flag_cleaned == 1) | (flag_cleaned == 2)]\n","\n","# Shift all labels down by one so that the same schema is being used for gmm_labels and esa_labels (e.g. 0 = ice, 1 = lead)\n","esa_labels = esa_labels - 1"],"metadata":{"id":"BtNhaLfXyoCE","executionInfo":{"status":"aborted","timestamp":1708441949592,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n","\n","# Replace 'y_true' and 'y_pred' with your actual and predicted labels\n","y_true = esa_labels\n","y_pred = gmm_labels\n","\n","# Compute confusion matrix\n","cm = confusion_matrix(y_true, y_pred)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_true, y_pred)\n","\n","# Display classification report\n","report = classification_report(y_true, y_pred)\n","print(\"Classification Report:\\n\", report)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(8, 6))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()\n","\n","# Display accuracy\n","print(f\"Accuracy: {accuracy:.2f}\")\n"],"metadata":{"id":"a-cPUflu6P4N","executionInfo":{"status":"aborted","timestamp":1708441949592,"user_tz":0,"elapsed":6,"user":{"displayName":"Fiona M","userId":"08618784324952634942"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, the GMM classification is 76% accurate against the ESA classification."],"metadata":{"id":"coOymLMo-1xc"}}],"metadata":{"colab":{"gpuType":"T4","provenance":[{"file_id":"1-9KuNC5bQct9Of-CgnBmjJrrof3kPDxA","timestamp":1708428333787}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"vscode":{"interpreter":{"hash":"ea7d6a51bd5bebc5530766074a327a4db30535c2e45ca3d8f95c2d659fc0ffa4"}}},"nbformat":4,"nbformat_minor":0}